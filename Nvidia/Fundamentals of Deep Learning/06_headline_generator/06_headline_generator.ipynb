{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Reading the Data\n",
    "We'll start by reading in all the headlines from the articles. The articles are in CVS format, so we use *pandas* to read them in."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "9335"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "nyt_dir = 'data/nyt_dataset/articles/'\n",
    "\n",
    "all_headlines = []\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # Read in all the data from csv\n",
    "        headlines_df = pd.read_csv(nyt_dir + filename)\n",
    "        # Add all the headlines to our list\n",
    "        all_headlines.extend(list(headlines_df.headline.values)) #todo lookup .extend\n",
    "len(all_headlines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['Initial Description',\n 'Rough Estimates',\n 'El Pasatiempo Nacional',\n 'Cooling Off on a Hot Day at Yankee Stadium',\n 'Trump’s Staff Mixed Politics and Paydays',\n 'A Virtuoso Rebuilding Act Requires Everyone in Tune',\n '‘Homeland,’ Season 6, Episode 11: Is Quinn Just a Natural Killer?',\n '‘Big Little Lies’ and the Art of Empathy',\n 'Upending a Whodunit',\n '‘Feud: Bette and Joan’ Episode 5: Taking the Stage',\n '‘Billions’ Season 2, Episode 7: Greed Is Good. Except When It’s Not.',\n 'Unknown',\n 'What’s Going On in This Picture? | April 3, 2017',\n 'Unknown',\n 'Have You Ever Felt Pressured by Family or Others in Making an Important Decision About Your Future?',\n 'Unknown',\n 'A Cornerstone of Peace at Risk',\n 'Trump Is  Wimping Out on Trade',\n 'The Dwindling Odds of Coincidence',\n 'What Was Lenin Thinking?']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[20:40]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "8603"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = [h for h in all_headlines if h != 'Unknown'] # TODO lup\n",
    "len(all_headlines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also want to remove punctuation and make our sentences all lower case, because this will make our model easier to train. For our purposes, there is little or no difference between a line ending with \"!\" or \"?\" or whether words are capitalized, as in \"The\" or lower-case, as in \"the\". With fewer unique tokens, our model will be easier to train.\n",
    "# Tokenization\n",
    "```python\n",
    "tensorflow.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "    split=' ', char_level=False, oov_token=None, document_count=0, **kwargs\n",
    ")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11753\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the words in our headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can take a quick look at word_index dictionary to see how the tokenizer saves the words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 82, 'man': 138, 'panama': 3379, 'canal': 7144}\n"
     ]
    }
   ],
   "source": [
    "# Print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "               if key in ['a','man','a','plan','a','canal','panama']}\n",
    "print(subset_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[[2], [138], [2], [82], [2], [7144], [3379]]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[1]]) # 1st word met by tokenizer\n",
    "tokenizer.texts_to_sequences(['a','man','a','plan','a','canal','panama'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Sequences\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an', 'finding an expansive', 'finding an expansive view', 'finding an expansive view of', 'finding an expansive view of a', 'finding an expansive view of a forgotten', 'finding an expansive view of a forgotten people', 'finding an expansive view of a forgotten people in', 'finding an expansive view of a forgotten people in niger']\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[403, 17],\n [403, 17, 5242],\n [403, 17, 5242, 543],\n [403, 17, 5242, 543, 4],\n [403, 17, 5242, 543, 4, 2],\n [403, 17, 5242, 543, 4, 2, 1616],\n [403, 17, 5242, 543, 4, 2, 1616, 151],\n [403, 17, 5242, 543, 4, 2, 1616, 151, 5],\n [403, 17, 5242, 543, 4, 2, 1616, 151, 5, 1992]]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to sequence of tokens\n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # Convert our headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    # Create a series of sequences for each headline\n",
    "    for i in range(1,len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:9]))\n",
    "input_sequences[:9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Padding Sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n       403,  17])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "#Determine max sequence length\n",
    "max_sequence_length =  max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_sequence_length, padding='pre'))\n",
    "input_sequences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "['finding an']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[403,17]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Predictions and Target\n",
    "We also want to split our sequences into predictions and a target. The last words of the sequence will be our target, and the first words of the sequence will be our predictors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  17, 5242,  543,    4,    2])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moving though data\n",
    "'''\n",
    "input_sequences[-1] gives us last row\n",
    "input_sequences[:,-1] gives us last column\n",
    "[:] -> means we want columns not rows [,-1] => means we want last column\n",
    "'''\n",
    "# Predictors are every word expect the last\n",
    "predictors = input_sequences[:,:-1]\n",
    "# Labels are the last word\n",
    "labels = input_sequences[:,-1]\n",
    "labels[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like our earlier sections, these targets are categorical. We are predicting one word out of our possible total vocabulary. Instead of the network predicting scalar numbers, we will have it predict binary categories."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the Model\n",
    "For our model we're going to use a couple of new layers to deal with our sequential data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding of the model\n",
    "Can be found in [headline_gen_model_understanding.ipynb](https://github.com/GalaxUniv/Learning-Data/tree/main/Nvidia/Fundamentals%20of%20Deep%20Learning/06_headline_generator/headline_gen_model_understanding.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 27, 10)            117530    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11753)             1187053   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,348,983\n",
      "Trainable params: 1,348,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "input_len = max_sequence_length - 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input embedding layer\n",
    "model.add(Embedding(total_words, 10,input_length = input_len))\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add model output\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compiling Model\n",
    "We are going to select a particular optimizer that is well suited for LSTM tasks, called Adam optimizer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the Model\n",
    "We will train the model for 30 epochs,which will take a few minutes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1666/1666 [==============================] - 76s 42ms/step - loss: 7.8874\n",
      "Epoch 2/30\n",
      "1666/1666 [==============================] - 52s 31ms/step - loss: 7.4818\n",
      "Epoch 3/30\n",
      "1666/1666 [==============================] - 40s 24ms/step - loss: 7.2933\n",
      "Epoch 4/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 7.0908\n",
      "Epoch 5/30\n",
      "1666/1666 [==============================] - 43s 26ms/step - loss: 6.8736\n",
      "Epoch 6/30\n",
      "1666/1666 [==============================] - 40s 24ms/step - loss: 6.6387\n",
      "Epoch 7/30\n",
      "1666/1666 [==============================] - 40s 24ms/step - loss: 6.3946\n",
      "Epoch 8/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 6.1621\n",
      "Epoch 9/30\n",
      "1666/1666 [==============================] - 44s 27ms/step - loss: 5.9363\n",
      "Epoch 10/30\n",
      "1666/1666 [==============================] - 44s 26ms/step - loss: 5.7116\n",
      "Epoch 11/30\n",
      "1666/1666 [==============================] - 47s 28ms/step - loss: 5.4977\n",
      "Epoch 12/30\n",
      "1666/1666 [==============================] - 45s 27ms/step - loss: 5.2904\n",
      "Epoch 13/30\n",
      "1666/1666 [==============================] - 47s 28ms/step - loss: 5.0967\n",
      "Epoch 14/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 4.9098\n",
      "Epoch 15/30\n",
      "1666/1666 [==============================] - 44s 27ms/step - loss: 4.7302\n",
      "Epoch 16/30\n",
      "1666/1666 [==============================] - 45s 27ms/step - loss: 4.5668\n",
      "Epoch 17/30\n",
      "1666/1666 [==============================] - 45s 27ms/step - loss: 4.4022\n",
      "Epoch 18/30\n",
      "1666/1666 [==============================] - 43s 26ms/step - loss: 4.2527\n",
      "Epoch 19/30\n",
      "1666/1666 [==============================] - 48s 29ms/step - loss: 4.1132\n",
      "Epoch 20/30\n",
      "1666/1666 [==============================] - 47s 28ms/step - loss: 3.9781\n",
      "Epoch 21/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 3.8568\n",
      "Epoch 22/30\n",
      "1666/1666 [==============================] - 43s 26ms/step - loss: 3.7391\n",
      "Epoch 23/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 3.6337\n",
      "Epoch 24/30\n",
      "1666/1666 [==============================] - 43s 26ms/step - loss: 3.5278\n",
      "Epoch 25/30\n",
      "1666/1666 [==============================] - 41s 24ms/step - loss: 3.4361\n",
      "Epoch 26/30\n",
      "1666/1666 [==============================] - 42s 25ms/step - loss: 3.3373\n",
      "Epoch 27/30\n",
      "1666/1666 [==============================] - 44s 26ms/step - loss: 3.2555\n",
      "Epoch 28/30\n",
      "1666/1666 [==============================] - 47s 28ms/step - loss: 3.1774\n",
      "Epoch 29/30\n",
      "1666/1666 [==============================] - 51s 31ms/step - loss: 3.1019\n",
      "Epoch 30/30\n",
      "1666/1666 [==============================] - 50s 30ms/step - loss: 3.0320\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x13f5cb3cfd0>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors,labels,epochs=30,verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussion of Results\n",
    "We can see that loss decreased over the course of training. We could train our model further to decrees the loss, but that would take some time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "[7107]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen= max_sequence_length - 1, padding='pre')\n",
    "    prediction = [np.argmax(model.predict(token_list,verbose=0))]\n",
    "    return prediction\n",
    "pred = predict_next_token('today in new york')\n",
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "['subway’s']"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([pred])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate New Headlines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def generate_new_headlines(seed_text,next_words = 1):\n",
    "    for _ in range(next_words):\n",
    "        # Predict next token\n",
    "        prediction =predict_next_token(seed_text)\n",
    "        # Convert token to words\n",
    "        next_word = tokenizer.sequences_to_texts([prediction])[0]\n",
    "        # Add next word to the seed_text.\n",
    "        seed_text += \" \" + next_word\n",
    "    return seed_text.title()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets try it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington Dc Is Moscow For A Hike On Wall\n",
      "Today In New York Subway’S The Cadaver A 1 P\n",
      "The School District Has The Odd It Just All It\n",
      "Crime Has Become A Singular Task Of The Newspaper\n"
     ]
    }
   ],
   "source": [
    "seed_text = [\n",
    "    'washington dc is',\n",
    "    'today in new york',\n",
    "    'the school district has',\n",
    "    'crime has become'\n",
    "]\n",
    "for seed in seed_text:\n",
    "    print(generate_new_headlines(seed,next_words=6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "The result may be a bit underwhelming after 30 epochs of training. We can notice that most of the headlines make some kind of grammatical sense, but don't necessarily indicate a good contextual understanding. The results might improve somewhat by running more epochs.\n",
    "\n",
    "### Other improvements\n",
    "We could try using pretrained embeddings with Word2Vec or GloVe, rather than learning them during training as we did with the Keras Embedding layer.\n",
    "\n",
    "### Ultimately\n",
    "NLP has moved beyond simple LSTM model to Transformer-based-pre-trained models, which are able to learn language from context from huge amounts of textual data such as Wikipedia. These pre-trained models are then used as a starting point for transfer learning to solve NLP tasks such as the one we just tried for text completion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the Data\n",
    "We'll start by reading in all the headlines from the articles. The articles are in CVS format, so we use *pandas* to read them in."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "9335"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "nyt_dir = 'data/nyt_dataset/articles/'\n",
    "\n",
    "all_headlines = []\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # Read in all the data from csv\n",
    "        headlines_df = pd.read_csv(nyt_dir + filename)\n",
    "        # Add all the headlines to our list\n",
    "        all_headlines.extend(list(headlines_df.headline.values)) #todo lookup .extend\n",
    "len(all_headlines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['Initial Description',\n 'Rough Estimates',\n 'El Pasatiempo Nacional',\n 'Cooling Off on a Hot Day at Yankee Stadium',\n 'Trump’s Staff Mixed Politics and Paydays',\n 'A Virtuoso Rebuilding Act Requires Everyone in Tune',\n '‘Homeland,’ Season 6, Episode 11: Is Quinn Just a Natural Killer?',\n '‘Big Little Lies’ and the Art of Empathy',\n 'Upending a Whodunit',\n '‘Feud: Bette and Joan’ Episode 5: Taking the Stage',\n '‘Billions’ Season 2, Episode 7: Greed Is Good. Except When It’s Not.',\n 'Unknown',\n 'What’s Going On in This Picture? | April 3, 2017',\n 'Unknown',\n 'Have You Ever Felt Pressured by Family or Others in Making an Important Decision About Your Future?',\n 'Unknown',\n 'A Cornerstone of Peace at Risk',\n 'Trump Is  Wimping Out on Trade',\n 'The Dwindling Odds of Coincidence',\n 'What Was Lenin Thinking?']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[20:40]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "8603"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = [h for h in all_headlines if h != 'Unknown'] # TODO lup\n",
    "len(all_headlines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also want to remove punctuation and make our sentences all lower case, because this will make our model easier to train. For our purposes, there is little or no difference between a line ending with \"!\" or \"?\" or whether words are capitalized, as in \"The\" or lower-case, as in \"the\". With fewer unique tokens, our model will be easier to train.\n",
    "# Tokenization\n",
    "```python\n",
    "tensorflow.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "    split=' ', char_level=False, oov_token=None, document_count=0, **kwargs\n",
    ")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11753\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the words in our headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can take a quick look at word_index dictionary to see how the tokenizer saves the words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 82, 'man': 138, 'panama': 3379, 'canal': 7144}\n"
     ]
    }
   ],
   "source": [
    "# Print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "               if key in ['a','man','a','plan','a','canal','panama']}\n",
    "print(subset_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['in']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[1]]) # 1st word met by tokenizer\n",
    "tokenizer.texts_to_sequences(['a','man','a','plan','a','canal','panama'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a Sequences\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finding an', 'finding an expansive', 'finding an expansive view', 'finding an expansive view of', 'finding an expansive view of a', 'finding an expansive view of a forgotten', 'finding an expansive view of a forgotten people', 'finding an expansive view of a forgotten people in', 'finding an expansive view of a forgotten people in niger']\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[403, 17],\n [403, 17, 5242],\n [403, 17, 5242, 543],\n [403, 17, 5242, 543, 4],\n [403, 17, 5242, 543, 4, 2],\n [403, 17, 5242, 543, 4, 2, 1616],\n [403, 17, 5242, 543, 4, 2, 1616, 151],\n [403, 17, 5242, 543, 4, 2, 1616, 151, 5],\n [403, 17, 5242, 543, 4, 2, 1616, 151, 5, 1992]]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to sequence of tokens\n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # Convert our headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    # Create a series of sequences for each headline\n",
    "    for i in range(1,len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:9]))\n",
    "input_sequences[:9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Padding Sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n       403,  17])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "#Determine max sequence length\n",
    "max_sequence_length =  max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_sequence_length, padding='pre'))\n",
    "input_sequences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
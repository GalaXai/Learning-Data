{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## The Dataset\n",
    "The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification)\n",
    "# Loading the Model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape = (224,224,3),\n",
    "    include_top=False\n",
    ")\n",
    "base_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Freezing the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding new layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,717,766\n",
      "Trainable params: 3,078\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(224,224,3))\n",
    "# Separately from setting trainable on the model, we set training to False\n",
    "x = base_model(inputs, training = False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs,outputs)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compiling the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, metrics =['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Augmenting the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "    samplewise_center = 0,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range =0.1,\n",
    "    height_shift_range =0.1,\n",
    "    horizontal_flip = True,\n",
    "    vertical_flip = True,\n",
    ")\n",
    "datagen_valid = ImageDataGenerator(samplewise_center = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10901 images belonging to 6 classes.\n",
      "Found 2698 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and iterate training dataset\n",
    "train_it = datagen_train.flow_from_directory(\n",
    "    'dataset/train/',\n",
    "    target_size=(224,224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    ")\n",
    "# Load and iterate validation dataset\n",
    "valid_it = datagen_valid.flow_from_directory(\n",
    "    'dataset/test/',\n",
    "    target_size=(224,224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 209s 6s/step - loss: 2.7039 - accuracy: 0.4455 - val_loss: 1.7285 - val_accuracy: 0.5764\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 196s 6s/step - loss: 1.1061 - accuracy: 0.6718 - val_loss: 1.0915 - val_accuracy: 0.6632\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 199s 6s/step - loss: 0.7131 - accuracy: 0.7866 - val_loss: 0.6060 - val_accuracy: 0.8125\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 227s 7s/step - loss: 0.4842 - accuracy: 0.8482 - val_loss: 0.5623 - val_accuracy: 0.8264\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 234s 7s/step - loss: 0.4147 - accuracy: 0.8696 - val_loss: 0.4421 - val_accuracy: 0.8715\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 242s 7s/step - loss: 0.3509 - accuracy: 0.8873 - val_loss: 0.4791 - val_accuracy: 0.8646\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 233s 7s/step - loss: 0.2798 - accuracy: 0.9008 - val_loss: 0.4448 - val_accuracy: 0.8750\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 241s 7s/step - loss: 0.2991 - accuracy: 0.9107 - val_loss: 0.3231 - val_accuracy: 0.8889\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 248s 7s/step - loss: 0.2282 - accuracy: 0.9223 - val_loss: 0.3568 - val_accuracy: 0.8924\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 294s 9s/step - loss: 0.2244 - accuracy: 0.9187 - val_loss: 0.1907 - val_accuracy: 0.9306\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x216f83fc220>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size*0.1,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size*0.1,\n",
    "          epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unfreeze the Model for Fine-Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = .00001),\n",
    "              loss = keras.losses.categorical_crossentropy , metrics = ['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 930s 26s/step - loss: 0.3110 - accuracy: 0.9045 - val_loss: 0.2096 - val_accuracy: 0.9236\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 889s 26s/step - loss: 0.1563 - accuracy: 0.9563 - val_loss: 0.3943 - val_accuracy: 0.8576\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 871s 26s/step - loss: 0.1006 - accuracy: 0.9705 - val_loss: 0.0887 - val_accuracy: 0.9583\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 860s 25s/step - loss: 0.0950 - accuracy: 0.9732 - val_loss: 0.1670 - val_accuracy: 0.9479\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 878s 26s/step - loss: 0.0459 - accuracy: 0.9830 - val_loss: 0.1173 - val_accuracy: 0.9583\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 886s 26s/step - loss: 0.0711 - accuracy: 0.9741 - val_loss: 0.1168 - val_accuracy: 0.9653\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 904s 27s/step - loss: 0.0574 - accuracy: 0.9821 - val_loss: 0.0730 - val_accuracy: 0.9757\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 902s 26s/step - loss: 0.0537 - accuracy: 0.9812 - val_loss: 0.3504 - val_accuracy: 0.9236\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 894s 26s/step - loss: 0.0415 - accuracy: 0.9848 - val_loss: 0.0844 - val_accuracy: 0.9722\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 930s 27s/step - loss: 0.0259 - accuracy: 0.9911 - val_loss: 0.0656 - val_accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x216f874e6d0>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size*0.1,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size*0.1,\n",
    "          epochs=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 454s 5s/step - loss: 0.0689 - accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.06890735030174255, 0.9770200252532959]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}